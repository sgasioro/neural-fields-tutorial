{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf5f230",
   "metadata": {},
   "source": [
    "# Neural Fields for 3D Reconstruction\n",
    "\n",
    "This notebook steps through the use of neural network representations for 3D reconstruction: namely, given a set of 2D camera images, we want to infer the structure of the 3D scene/object that produced the images. This problem is known as inverse rendering.\n",
    "\n",
    "\n",
    "To start, we need to generate some training data. We will be using a rendering tool developed at SLAC: [gradoptics](https://github.com/Magis-slac/gradoptics). \n",
    "\n",
    "Begin by installing all of the dependencies. \n",
    "\n",
    "**If running in Colab, change your runtime to GPU (Runtime>Change Runtime Type>Hardware Accelerator>GPU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbbd05",
   "metadata": {},
   "source": [
    "Basic dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9589b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --find-links https://download.pytorch.org/whl/torch_stable.html \"torch>=1.13.1+cu117\" numpy==1.23.1 matplotlib==3.5.2 scipy==1.8.1 tqdm==4.64.1 celluloid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcb797",
   "metadata": {},
   "source": [
    "gradoptics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradoptics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955bfc4",
   "metadata": {},
   "source": [
    "If running in Colab, need some files in the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sgasioro/neural-fields-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09639e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'neural-fields-tutorial/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddbf315",
   "metadata": {},
   "source": [
    "gradoptics is a ray-tracing simulator written in PyTorch. It uses the straight-line nature of light, as well as known interactions with optical elements, to render a scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45654aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradoptics as optics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f98c6",
   "metadata": {},
   "source": [
    "We start out with a few helper functions to set up a system of cameras. In all of the following we assume that the object we're interested in imaging is at the origin $(0, 0, 0)$. \n",
    "\n",
    "`gradoptics` has a default camera orientation (along the $+x$ axis). We would like to place arbitrary cameras in 3D space and orient them to point at the object of interest (at the origin). The first function, `point_to_origin`, gives the appropriate Euler angle rotation to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_to_origin(cam_position):\n",
    "    # Default is point along positive x\n",
    "    current_dir = torch.tensor([1., 0., 0.]).type(cam_position.dtype)\n",
    "    \n",
    "    # Vector pointing towards the origin\n",
    "    new_dir = -cam_position/torch.norm(cam_position)\n",
    "    \n",
    "    # Get a perpendicular axis, handling axis aligned cases\n",
    "    if torch.allclose(torch.abs(new_dir), torch.abs(current_dir)):\n",
    "        axis = torch.tensor([0., 0., 1.]).double()\n",
    "    else:\n",
    "        axis = torch.cross(current_dir, new_dir)\n",
    "     \n",
    "    # Normalize\n",
    "    axis *= 1/torch.norm(axis)\n",
    "    \n",
    "    # Get rotation angle via dot product\n",
    "    angle = torch.acos(torch.dot(current_dir,new_dir)/(torch.norm(current_dir)*torch.norm(new_dir)))\n",
    "    \n",
    "    # Get Euler angles from rotation vector using scipy (right handed coordinate system)\n",
    "    theta_x, theta_y, theta_z = R.from_rotvec(axis*angle).as_euler('xyz')\n",
    "    \n",
    "    # Gradoptics rotations need left handed coordinate system -- flip sign on y\n",
    "    return theta_x, -theta_y, theta_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5473c",
   "metadata": {},
   "source": [
    "We'll be operating at a fixed magnification, but adjusting camera positions and focal lengths. We can use the lensmakers formula to keep everything consistent/in focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494fe9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_focal_length(m, obj_distance):\n",
    "    f =  obj_distance / ((1 / m) + 1)\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b033102",
   "metadata": {},
   "source": [
    "`gradoptics` works in terms of `Scene` objects and optical elements within them. We can add multiple cameras to a scene, where a camera is a `Lens` and a `Sensor`. For this tutorial, we've fixed a magnification of `m=0.1` for all cameras and a sensor size of $200 \\times 200$ pixels. \n",
    "\n",
    "`setup_scene` places an arbitrary number of cameras pointing at the origin in a scene, given sets of spherical coordinates (`thetas` for polar angles from the $+z$ axis, `phis` for azimuthal angles, and `rs` for radii, in meters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_scene(scene, thetas=[], phis=[], rs=[]):\n",
    "    #Make sure the number of cameras matches between arguments\n",
    "    assert len(thetas) == len(phis), \"thetas and phis should have the same length\"\n",
    "    assert len(rs) == len(thetas), \"rs and thetas should have the same length\"\n",
    "    \n",
    "    n_cameras = len(thetas)\n",
    "    \n",
    "    # Given distance from object and magnification, calculate focal length for in focus object\n",
    "    m = 0.1\n",
    "    f = calculate_focal_length(m, rs[0])\n",
    "    \n",
    "    # Numerical aperture (size of camera opening, f-number)\n",
    "    na = 1/1.4\n",
    "\n",
    "    # Loop over cameras to add to scene\n",
    "    for i_cam in range(n_cameras):\n",
    "        # Avoid singular point with slight offset\n",
    "        if thetas[i_cam] == 0:\n",
    "            thetas[i_cam] = 1e-6\n",
    "            \n",
    "        # Get cartesian coordinates from spherical\n",
    "        x_cam = rs[i_cam]*np.sin(thetas[i_cam])*np.cos(phis[i_cam])\n",
    "        y_cam = rs[i_cam]*np.sin(thetas[i_cam])*np.sin(phis[i_cam])\n",
    "        z_cam = rs[i_cam]*np.cos(thetas[i_cam])\n",
    "        \n",
    "        cam_pos = torch.tensor([x_cam, y_cam, z_cam])\n",
    "\n",
    "        # Get orientation to point at origin and apply to lens\n",
    "        angles = point_to_origin(cam_pos)\n",
    "        transform = optics.simple_transform.SimpleTransform(*angles, cam_pos)\n",
    "        lens = optics.PerfectLens(f=f, m=m, na=na,\n",
    "                                  position = cam_pos,\n",
    "                                  transform = transform)\n",
    "\n",
    "        # Sensor position from lensmakers equation, rotated to match lens\n",
    "        rel_position = torch.tensor([-f * (1 + m), 0, 0])                       \n",
    "        rot_position = torch.matmul(transform.transform.float(), torch.cat((rel_position, torch.tensor([0]))))\n",
    "\n",
    "        sensor_position = cam_pos + rot_position[:-1]\n",
    "        viewing_direction = torch.matmul(transform.transform.float(), torch.tensor([1.,0,0,0]))\n",
    "\n",
    "        sensor = optics.Sensor(position=sensor_position, viewing_direction=tuple(viewing_direction.numpy()),\n",
    "                               resolution=(200,200), pixel_size=(2.4e-06, 2.4e-06),\n",
    "                               poisson_noise_mean=2.31, quantum_efficiency=0.72)\n",
    "        \n",
    "        # Add sensor and lens to the scene\n",
    "        scene.add_object(sensor)\n",
    "        scene.add_object(lens)\n",
    "        \n",
    "    return scene, n_cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb16e4a",
   "metadata": {},
   "source": [
    "## Example scene/training data\n",
    "\n",
    "One useful physics context for a 3D reconstruction is the imaging of clouds of atoms. This will be the data that we generate/use for this tutorial.\n",
    "\n",
    "We here set up a scene with a 1mm Gaussian atom cloud at the origin, and three cameras pointing at it: one along the x-axis, one along the y-axis, and one along the z-axis. For later use, we define a spherical region of interest around the origin with radius 5mm, which will cover all the light that would be captured on our given sensor/be sufficiently large for our atom cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = optics.Scene(optics.LightSourceFromDistribution(optics.AtomCloud(position=[0., 0., 0.])))\n",
    "scene, n_cameras = setup_scene(scene, [np.pi/2, np.pi/2, 0.], [0., np.pi/2., 0.], [5e-2]*3)\n",
    "\n",
    "scene.light_source.bounding_shape = optics.BoundingSphere(radii=0.005, \n",
    "                                                         xc=0, yc=0, zc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29cc58",
   "metadata": {},
   "source": [
    "Visualizing the scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "scene.plot(ax)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2020e8b",
   "metadata": {},
   "source": [
    "## Rendering and backward ray tracing\n",
    "\n",
    "Now that we have our scene and our cameras, we want to generate some images! We will do so using _backward ray tracing_.\n",
    "\n",
    "In _forward ray tracing_, light is emitted in all directions from a given light source, and we keep track of the light that hits our camera sensors. This nicely corresponds to physical intuition, but is inefficient: our cameras don't cover a large area of space, so many light rays need to be generated to get good images.\n",
    "\n",
    "In _backward ray tracing_, we say that we only care about light that hits pixels on our camera sensors. Since the light rays translate through our system along the same paths moving forward or backward in time, we instead generate rays at each of our camera pixels. By _integrating_ along these light rays through the object of interest, we can calculate the contribution of a light source to each pixel. \n",
    "\n",
    "\n",
    "To do this backward ray tracing, we need to generate appropriate rays. A ray is defined by an origin $\\mathbf{o}$ and a direction $\\mathbf{d}$, with a given point along a ray at time $t$ given by\n",
    "\\begin{equation}\n",
    "\\mathbf{r}(t) = \\mathbf{o} + t\\cdot \\mathbf{d}\n",
    "\\end{equation}\n",
    "\n",
    "We therefore first need to get coordinates of each pixel (the origins of each of our rays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ffc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_coords(sensor):\n",
    "    # Pixel coordinates in camera space\n",
    "    x = torch.linspace(-sensor.pixel_size[0]*sensor.resolution[0]/2 + sensor.pixel_size[0]/2,\n",
    "                         sensor.pixel_size[0]*sensor.resolution[0]/2 - sensor.pixel_size[0]/2, \n",
    "                         sensor.resolution[0])\n",
    "\n",
    "    y = torch.linspace(-sensor.pixel_size[1]*sensor.resolution[1]/2 + sensor.pixel_size[1]/2,\n",
    "                         sensor.pixel_size[1]*sensor.resolution[1]/2 - sensor.pixel_size[1]/2, \n",
    "                         sensor.resolution[1])\n",
    "    \n",
    "    pix_x, pix_y = torch.meshgrid(x, y)\n",
    "    \n",
    "    pix_z = torch.zeros((sensor.resolution[0], sensor.resolution[1]))\n",
    "    \n",
    "    all_coords = torch.stack([pix_x, pix_y, pix_z], dim=-1).reshape((-1, 3)).double()\n",
    "    \n",
    "    # Use transforms from above setup to go from pixel space to real (world) space\n",
    "    return sensor.c2w.apply_transform_(all_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279e8cb",
   "metadata": {},
   "source": [
    "To complete the rays, we then need a set of directions. For simplicity, we here consider the case of a pinhole camera (all the rays start from pixels and pass through the center of the lens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb14df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays_pinhole(sensor, lens, nb_rays=None, ind=None, device='cuda', return_ind=False):\n",
    "    \n",
    "    if ind is None:\n",
    "        #Set up for ray batching -- nb_rays switches between all pinhole rays or random batches\n",
    "        if nb_rays == None or nb_rays == sensor.resolution[0]*sensor.resolution[1]:\n",
    "            ind = torch.arange(0, sensor.resolution[0]*sensor.resolution[1])\n",
    "        else:\n",
    "            ind = torch.randint(0, sensor.resolution[0]*sensor.resolution[1], (nb_rays,))\n",
    "    \n",
    "    # Get origins\n",
    "    all_pix_coords = get_pixel_coords(sensor)  \n",
    "    origins = all_pix_coords[ind]\n",
    "    \n",
    "    #Get directions to center of lens\n",
    "    lens_center = lens.transform.transform[:-1, -1]\n",
    "    \n",
    "    directions = optics.batch_vector(lens_center[None, 0] - origins[:, 0],\n",
    "                                     lens_center[None, 1] - origins[:, 1],\n",
    "                                     lens_center[None, 2] - origins[:, 2]).type(origins.dtype)\n",
    "\n",
    "    # Set up rays\n",
    "    rays_sensor_to_lens = optics.Rays(origins, directions, device=device)\n",
    "    \n",
    "    if return_ind:\n",
    "        return rays_sensor_to_lens, ind\n",
    "    else:\n",
    "        return rays_sensor_to_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a38505",
   "metadata": {},
   "source": [
    "Backward ray tracing relies on doing a line integral along each ray. We do so by sampling a set of points along each ray and summing up the contributions at each of those points. Here we use a _hierarchical sampling_, which first spaces a set of points uniformly along the ray, and then does an importance sampling of additional points with notable density to \"focus\" on regions of interest. We do a stratified sampling on top of this, perturbing the uniformly spaced points to better approximate a continuous integral.\n",
    "\n",
    "Our object of interest is a transparent atom cloud, meaning that we only integrate the density: the contribution of a ray to a given pixel intensity is given by:\n",
    "\\begin{equation}\n",
    "C(\\mathbf{r}) \\propto \\int \\sigma(\\mathbf{r}(t))dt\n",
    "\\end{equation}\n",
    "\n",
    "For other contexts (such as [NeRF](https://www.matthewtancik.com/nerf)), these integrals include a view (ray direction) dependent color as well as a transmittance term (the probability of a light ray terminating as it moves through a given density).\n",
    "\n",
    "\n",
    "The integration is done only within the spherical region of interest that we defined above. Generating images then is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an integrator\n",
    "from gradoptics.integrator import HierarchicalSamplingIntegrator\n",
    "\n",
    "# 32 uniformly spaced points, 32 additional points\n",
    "integrator = HierarchicalSamplingIntegrator(32, 32)\n",
    "\n",
    "# Loop over cameras\n",
    "targets = []\n",
    "for i_cam in tqdm(range(n_cameras)):\n",
    "    # Generate rays for each camera (all pixels at once). i_cam*2 gives all sensor idxs, lenses are i_cam*2+1\n",
    "    incident_rays = get_rays_pinhole(scene.objects[i_cam*2], scene.objects[i_cam*2+1])\n",
    "    \n",
    "    # Trace rays through the scene (includes the integration)\n",
    "    intensities = optics.backward_ray_tracing(incident_rays, scene, \n",
    "                                                scene.light_source, integrator, max_iterations=3)\n",
    "    \n",
    "    # Store the result for a given camera\n",
    "    targets.append(intensities.cpu().clone().reshape(scene.objects[i_cam*2].resolution))\n",
    "    \n",
    "    del intensities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23588964",
   "metadata": {},
   "source": [
    "## Resulting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7869964",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for i in range(len(targets)):\n",
    "    plt.imshow(targets[i].T, origin='lower')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c025462",
   "metadata": {},
   "source": [
    "## 3D reconstruction\n",
    "\n",
    "We now have some \"captured\" data and we know the optical setup. Assuming we know only that, can we figure out what our atom cloud looks like in 3D?\n",
    "\n",
    "We do so using an analysis-by-synthesis approach: we start out with some initial guess of the 3D structure, simulate a set of images as we did above, compare to our \"captured\" data, and adjust our 3D model to minimize the difference between the guess and the data.\n",
    "\n",
    "Here, our 3D model is parametrized by a class of neural networks called SIRENs. This is not a unique choice! But it works well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785a92d",
   "metadata": {},
   "source": [
    "Within `gradoptics`, we can set a neural network as a light source and do rendering in exactly the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9154912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.siren import Siren\n",
    "import torch.nn as nn\n",
    "from gradoptics import LightSourceFromNeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b520e",
   "metadata": {},
   "source": [
    "First we define the SIREN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30cda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "in_features = 3\n",
    "hidden_features = 64\n",
    "hidden_layers = 3\n",
    "out_features = 1\n",
    "\n",
    "model = Siren(in_features, hidden_features, hidden_layers, out_features,\n",
    "              outermost_linear=True, outermost_linear_activation=nn.Softplus()).double().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e6181",
   "metadata": {},
   "source": [
    "Then we set up a scene for training. This has all of the same cameras as the scene we used above, but the light source is now a neural network with learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN light source using slightly tighter bounding sphere\n",
    "nn_light_source = LightSourceFromNeuralNet(model, optics.BoundingSphere(radii=0.003, \n",
    "                                                                           xc=0, yc=0, zc=0),\n",
    "                                                    rad=0.003, x_pos=0)\n",
    "scene_train = optics.Scene(nn_light_source)\n",
    "\n",
    "# Add sensors/lenses from scene above\n",
    "for i in range(len(scene.objects)):\n",
    "    scene_train.add_object(scene.objects[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365cdcb",
   "metadata": {},
   "source": [
    "We do a batch gradient descent with each batch drawn from a given camera sensor. We'll do a quick training for demonstration. Loss values are large due to fairly large light collection. Training may take a bit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 4096\n",
    "\n",
    "# Loss function -- mean squared error between pixels\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Same integrator as above\n",
    "integrator = HierarchicalSamplingIntegrator(32, 32)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(scene_train.light_source.network.parameters(), lr=1e-3)\n",
    "\n",
    "# Keep track of losses\n",
    "losses = []\n",
    "\n",
    "for i_iter in tqdm(range(2000)):\n",
    "    # Choose a random camera\n",
    "    i_cam = torch.randint(n_cameras, (1,))\n",
    "    \n",
    "    # Grab a random batch of rays\n",
    "    rays, ind = get_rays_pinhole(scene.objects[i_cam*2], \n",
    "                                 scene.objects[i_cam*2+1], nb_rays=batch_size, device='cuda', return_ind=True)\n",
    "    \n",
    "    # Get corresponding pixels from target images\n",
    "    target_vals = targets[i_cam].flatten()[ind]\n",
    "    \n",
    "    # Ray trace using neural network light source\n",
    "    intensities = optics.backward_ray_tracing(rays, scene_train, \n",
    "                                             scene_train.light_source, integrator, max_iterations=3)\n",
    "    \n",
    "    # Calculate the loss -- 1e9 scaling is a result of unnormalized PDF in atom cloud\n",
    "    loss = loss_fn(intensities*1e9, target_vals.double().cuda())\n",
    "    \n",
    "    # Calculate gradients and update network parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Keep track of results\n",
    "    losses.append(loss.item())\n",
    "    if i_iter % 100 == 0:\n",
    "        print(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            torch.save(scene_train.light_source.network.state_dict(), \n",
    "                       f'model_{i_iter}iter_tutorial.pt')\n",
    "        with open('losses_tutorial.pkl', 'wb') as file:\n",
    "            pickle.dump(losses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f756b3",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "First, plotting the loss, it should go down. If the loss is not flat, training for longer will improve the quality of results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c325017",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0bb49",
   "metadata": {},
   "source": [
    "Then let's visualize the densities: first we define some sampled grid of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep square grid within training sphere\n",
    "rad = 0.003\n",
    "bound = 1/np.sqrt(3.)\n",
    "\n",
    "# 100 x 100 x 100 cube\n",
    "n_side=100\n",
    "\n",
    "# Inputs to NN model are centered and scaled\n",
    "grid = torch.cartesian_prod(torch.linspace(-bound, bound, n_side),\n",
    "                            torch.linspace(-bound, bound, n_side),\n",
    "                            torch.linspace(-bound, bound, n_side)).cuda().double()\n",
    "\n",
    "# But also define in real space\n",
    "grid_real = torch.cartesian_prod(torch.linspace(-rad*bound, rad*bound, n_side),\n",
    "                            torch.linspace(-rad*bound, rad*bound, n_side),\n",
    "                            torch.linspace(-rad*bound, rad*bound, n_side)).cuda().double()\n",
    "\n",
    "# Calculate model density from NN and \"true\" density from pdf\n",
    "with torch.no_grad():\n",
    "    densities = model(grid)[0].reshape((n_side, n_side, n_side)).cpu()\n",
    "    pdf_vals = scene.light_source.pdf(grid_real).reshape((n_side, n_side, n_side)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(8,3))\n",
    "\n",
    "ax[0, 0].text(0.9, 0.5, \"Reconstructed\", ha='right', fontsize=14)\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "ax[1, 0].text(0.9, 0.5, \"True\", ha='right', fontsize=14)\n",
    "ax[1, 0].axis('off')\n",
    "\n",
    "ax[0, 1].imshow(densities.sum(dim=0).T, origin=\"lower\")\n",
    "ax[0, 1].set_title('Sum x', fontsize=14)\n",
    "ax[0, 1].axis('off')\n",
    "ax[1, 1].imshow(pdf_vals.sum(dim=0).T, origin=\"lower\")\n",
    "ax[1, 1].axis('off')\n",
    "\n",
    "ax[0, 2].imshow(densities.sum(dim=1).T, origin=\"lower\")\n",
    "ax[0, 2].set_title('Sum y', fontsize=14)\n",
    "ax[0, 2].axis('off')\n",
    "ax[1, 2].imshow(pdf_vals.sum(dim=1).T, origin=\"lower\")\n",
    "ax[1, 2].axis('off')\n",
    "\n",
    "ax[0, 3].imshow(densities.sum(dim=2).T, origin=\"lower\")\n",
    "ax[0, 3].set_title('Sum z', fontsize=14)\n",
    "ax[0, 3].axis('off')\n",
    "ax[1, 3].imshow(pdf_vals.sum(dim=2).T, origin=\"lower\")\n",
    "ax[1, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f40e9",
   "metadata": {},
   "source": [
    "Then visualize as model trains by loading in saved models. `celluloid` is just a wrapper for matplotlib animations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pattern = 'model_*iter_tutorial.pt'\n",
    "n_checkpoints = len(glob(f_pattern))\n",
    "\n",
    "last = (n_checkpoints-1)*100\n",
    "\n",
    "all_densities = []\n",
    "with torch.no_grad():\n",
    "    for n_iter in tqdm(np.arange(0, last+1, 100)):\n",
    "        fname = f_pattern.replace(\"*iter\", \"{n_iter}iter\").format(n_iter=n_iter)\n",
    "        model.load_state_dict(torch.load(fname))\n",
    "        densities = model(grid)[0].reshape((n_side, n_side, n_side)).cpu()\n",
    "        all_densities.append(densities.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "fig, ax = plt.subplots(2, 4, figsize=(8,3))\n",
    "camera = Camera(fig)\n",
    "\n",
    "for i in range(len(all_densities)):\n",
    "    ax[0, 0].text(0.9, 0.5, \"Reconstructed\", ha='right', fontsize=14)\n",
    "    ax[0, 0].axis('off')\n",
    "\n",
    "    ax[1, 0].text(0.9, 0.5, \"True\", ha='right', fontsize=14)\n",
    "    ax[1, 0].axis('off')\n",
    "\n",
    "    ax[0, 1].imshow(all_densities[i].sum(dim=0).T, origin=\"lower\")\n",
    "    ax[0, 1].set_title('Sum x', fontsize=14)\n",
    "    ax[0, 1].axis('off')\n",
    "    ax[1, 1].imshow(pdf_vals.sum(dim=0).T, origin=\"lower\")\n",
    "    ax[1, 1].axis('off')\n",
    "\n",
    "    ax[0, 2].imshow(all_densities[i].sum(dim=1).T, origin=\"lower\")\n",
    "    ax[0, 2].set_title('Sum y', fontsize=14)\n",
    "    ax[0, 2].axis('off')\n",
    "    ax[1, 2].imshow(pdf_vals.sum(dim=1).T, origin=\"lower\")\n",
    "    ax[1, 2].axis('off')\n",
    "\n",
    "    ax[0, 3].imshow(all_densities[i].sum(dim=2).T, origin=\"lower\")\n",
    "    ax[0, 3].set_title('Sum z', fontsize=14)\n",
    "    ax[0, 3].axis('off')\n",
    "    ax[1, 3].imshow(pdf_vals.sum(dim=2).T, origin=\"lower\")\n",
    "    ax[1, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    camera.snap()\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save('tutorial_training.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d739e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(open('tutorial_training.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a200063",
   "metadata": {},
   "source": [
    "We can also generate a set of images as we move around in 3D (even where we haven't trained with cameras!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN light source using slightly tighter bounding sphere\n",
    "nn_light_source = LightSourceFromNeuralNet(model, optics.BoundingSphere(radii=0.003, \n",
    "                                                                           xc=0, yc=0, zc=0),\n",
    "                                                    rad=0.003, x_pos=0)\n",
    "\n",
    "all_azim_angles = np.linspace(0, 2*np.pi, 30)\n",
    "\n",
    "# Same integrator as above\n",
    "integrator = HierarchicalSamplingIntegrator(32, 32)\n",
    "\n",
    "batch_size = 4096\n",
    "\n",
    "with torch.no_grad():\n",
    "    interp_ims = []\n",
    "    for azim in tqdm(all_azim_angles):\n",
    "        scene_interp = optics.Scene(nn_light_source)\n",
    "\n",
    "        scene_interp, n_cameras = setup_scene(scene_interp, [np.pi/2], [azim], [5e-2])\n",
    "\n",
    "        all_ind = torch.arange(0, scene_interp.objects[0].resolution[0]*scene_interp.objects[0].resolution[1])\n",
    "        \n",
    "        ind_batches = all_ind.split(batch_size)\n",
    "        \n",
    "        all_intensities = []\n",
    "        for ind_batch in ind_batches:\n",
    "            # Generate rays for each camera (all pixels at once). i_cam*2 gives all sensor idxs, lenses are i_cam*2+1\n",
    "            incident_rays = get_rays_pinhole(scene_interp.objects[0], scene_interp.objects[1], ind=ind_batch)\n",
    "\n",
    "            # Trace rays through the scene (includes the integration)\n",
    "            intensities = optics.backward_ray_tracing(incident_rays, scene_interp, \n",
    "                                                        scene_interp.light_source, integrator, max_iterations=3)\n",
    "            \n",
    "            all_intensities.append(intensities.cpu().clone())\n",
    "            \n",
    "            del intensities\n",
    "            \n",
    "        all_intensities = torch.cat(all_intensities)\n",
    "\n",
    "        # Store the result for a given camera\n",
    "        interp_ims.append(all_intensities.cpu().clone().reshape(scene_interp.objects[0].resolution))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e4bd9",
   "metadata": {},
   "source": [
    "Do the same for the true object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81af6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_light_source = optics.LightSourceFromDistribution(optics.AtomCloud(position=[0., 0., 0.]))\n",
    "\n",
    "all_azim_angles = np.linspace(0, 2*np.pi, 30)\n",
    "\n",
    "# Same integrator as above\n",
    "integrator = HierarchicalSamplingIntegrator(32, 32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    interp_ims_true = []\n",
    "    for azim in tqdm(all_azim_angles):\n",
    "        scene_interp = optics.Scene(true_light_source)\n",
    "        scene_interp.light_source.bounding_shape = optics.BoundingSphere(radii=0.005, \n",
    "                                                         xc=0, yc=0, zc=0)\n",
    "\n",
    "        scene_interp, n_cameras = setup_scene(scene_interp, [np.pi/2], [azim], [5e-2])\n",
    "        \n",
    "        # Generate rays for each camera (all pixels at once). i_cam*2 gives all sensor idxs, lenses are i_cam*2+1\n",
    "        incident_rays = get_rays_pinhole(scene_interp.objects[0], scene_interp.objects[1])\n",
    "\n",
    "        # Trace rays through the scene (includes the integration)\n",
    "        intensities = optics.backward_ray_tracing(incident_rays, scene_interp, \n",
    "                                                    scene_interp.light_source, integrator, max_iterations=3)\n",
    "\n",
    "\n",
    "        # Store the result for a given camera\n",
    "        interp_ims_true.append(intensities.cpu().clone().reshape(scene_interp.objects[0].resolution))\n",
    "        \n",
    "        del intensities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab855d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(2, 2, figsize=(5, 4))\n",
    "camera = Camera(fig)\n",
    "\n",
    "for i in range(len(interp_ims)):\n",
    "    ax[0, 0].text(0.9, 0.5, \"Reconstructed\", ha='right', fontsize=14)\n",
    "    ax[0, 0].axis('off')\n",
    "\n",
    "    ax[1, 0].text(0.9, 0.5, \"True\", ha='right', fontsize=14)\n",
    "    ax[1, 0].axis('off')\n",
    "\n",
    "    ax[0, 1].imshow(interp_ims[i].T, origin='lower')\n",
    "    ax[1, 1].imshow(interp_ims_true[i].T, origin='lower')\n",
    "    camera.snap()\n",
    "    \n",
    "anim = camera.animate()\n",
    "anim.save('interp_around.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(open('interp_around.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba7ffc",
   "metadata": {},
   "source": [
    "Not perfect! Some reconstruction artifacts where we don't have information (diagonals, etc).\n",
    "\n",
    "Try adding in more cameras, training for longer -- see how results improve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02695cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
